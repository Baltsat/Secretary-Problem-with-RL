# secretary-problem-RL-DDQN + SNN

## Problem

Here we address a modification of [the secretary problem](https://en.wikipedia.org/wiki/Secretary_problem), where the goal is to maximize *the expectation value* for the score of the hired applicant.

![image](https://user-images.githubusercontent.com/42536677/183862240-f6e4534d-e741-4847-83e3-c8f3add71749.png)

В теории оптимизации остановки выбора широко известна так называемая задача о разборчивой невесте или проблема секретаря. В реальной жизни эта задача может представляться, например, при распределении органов доноров между нуждающимися или отправке аппаратов искусственной вентиляции легких в больницы. 
В простейшем случае проблемы секретаря весьма несложно и чисто математически выводится оптимальная стратегия. Однако, аналитическое решение не обладает всей гибкостью при усложнении начального условия, допустим, увеличением числа возможных действий агента. Поэтому применение алгоритмического решения в этой задаче обосновано. 
Применение машинного обучения в задачах оптимизации остановки выбора не ново. Ранее, Timo Welti и коллеги показали, что использование глубокого обучения позволяет решать оптимально по скорости и точности высоко-размерные задачи оптимизации остановки выбора, например, задачи ценообразования Американского или Бермудского пут-опционов. А Беккер и коллеги показали, что использование глубокого обучения с подкреплением в высоко-размерных задачах ценообразования Бермудского макс-колл опциона и оптимальной остановки дробного броуновского движения показывает очень точные результаты и высокую скорость работы.
В своей работе я расширил множество возможных действий агента в классической задаче о разборчивой невесте. Теперь, агент не только мог выбирать или пропускать текущего претендента, но и возвращаться к предыдущему с некоторой функцией потерь. Другими словами, теперь принцесса могла при желании догнать отвергнутого жениха и жить с ним долго, правда, уже не так счастливо. Интерпретация в нашем случае может выглядеть следующим образом: из-за потери времени мы несем чуть меньшую пользу госпиталю, в который мы отправляем аппарат ИВЛ сейчас, а не раньше, но в любом случае наши действия будут оправданы. 

## Ход работы
- На основе OpenAi Gym я создал среду, в которой реализовал стандартные методы и описаны действия агента. В этой среде все кандидаты имеют случайный вес от 0 до 1. 
- Реализована функция потерь при действиях агента по возвращению к предыдущему кандидату. Её я создал по аналогии с моделью Броуновского движения для рынка акций. Так, одно слагаемое было пропорционально квадрату весу кандидата с коэффициентом 0,2, а второе являлось случайным числом распределенным равномерно в 0.01 окрестности нуля. Стохастический член в функции потерь нужен, чтобы отразить возможные изменения цен кандидатов. Коэффициент 0.2 пропорционального коэффициента был подобран эмпирически. Функция выбрана квадратичная, так как она выпуклая. Хочется, чтобы потери от несвоевременного отказа от более дорого кандидата росли быстрее при бОльшей цене кандидата.

На графике черным и красным отмечены графики функции потерь и reward без учета стохастики в зависимости от цены кандидата.
![image](https://user-images.githubusercontent.com/42536677/183862376-9c56671b-0382-41d0-b8a4-2b9a00ca94c9.png)


- С использованием tensorflow и keras построил двух-слойную нейросеть двойного глубокого Q-обучения (DDQN). Я обучил агента используя 600 итераций обновления Q-функции. Ниже представлен график результативности агента в зависимости от количества итераций. 
![image](https://user-images.githubusercontent.com/42536677/183863022-f9bda403-bd95-4660-a726-59f4b42a93d3.png)
Видим, что в самом начале агент показывает результат 0.5, что логично, так как кандидаты имеют равномерно распределенный вес в интервале 0 и 1. Позже алгоритм достигает плато и в среднем показывает, как мы видим, результат выше 0.9.   Почему DDQN? Потому что Fathan и коллеги в 2021 году показали, что DDQN показывает бОльшую эффективность, чем С51 или IQN в решении задач оптимальной остановки.
- Симулировал поведение агента в 10.000 экспериментах по схеме Монте-Карло. 
- Визуализировал и проанализировал полученные результаты.

## Результаты
На графиках представлены гистограммы вероятностного распределения матожидания результата в двух случаях. В первом, функция потерь прямо пропорциональна весу кандидата с коэффициентом 0.2. Во втором, пропорциональность квадратичная с тем же коэффициентом. Видим, что алгоритм хорошо справляется с поставленной задачей и показывает результат 0.93 и 0.96 соответственно. Очевидно, что квадратичная зависимость показывает значительно лучший результат. Это связано именно со смещением распределения политики агента в сторону выбора действия по возвращению к предыдущему кандидату. 

![image](https://user-images.githubusercontent.com/42536677/183863128-66123040-105a-4daa-9964-f1e67b4a1da3.png)
Прямая пропорциональность

![image](https://user-images.githubusercontent.com/42536677/183863205-e4dc9c17-8e27-42b1-9b05-01bac0c9da76.png)
Квадратичная пропорциональность

На графиках представлены гистограммы вероятностного распределения матожидания результата в двух случаях. В первом, функция потерь прямо пропорциональна весу кандидата с коэффициентом 0.2. Во втором, пропорциональность квадратичная с тем же коэффициентом. Видим, что алгоритм хорошо справляется с поставленной задачей и показывает результат 0.93 и 0.96 соответственно. Очевидно, что квадратичная зависимость показывает значительно лучший результат. Это связано именно со смещением распределения политики агента в сторону выбора действия по возвращению к предыдущему кандидату. 
## Перспективы исследования
В перспективе, среду можно модифицировать и дальше. Первым шагом будет возможность агента брать не только последнего пропущенного кандидата, но любого из пропущенных. В этом случае будем использовать какую-либо функцию потерь от длины шага назад и оценки кандидата.  Вторым шагом мы предоставим агенту возможность получать информацию о кандидатах впереди. Например, за некоторую плату агент сможет получить информацию о дисперсии следующих нескольких кандидатов. Аналогом этого может служить то, что отдел распределения аппаратов ИВЛ с некоторыми издержками нанимает аналитиков, общается с заказчиками или строит модели для прогнозирования спроса в следующий момент времени.  Третьим шагом мы сделаем издержки от такого действия непрерывной, а не дискретной функцией от дисперсии. Это означает, что бОльшие средства вложенные в прогнозирование, принесут бОльшую точность прогноза.
В дальнейшем, результаты моего исследования возможно применить при построении торговых стратегий на основе алгоритмов глубокого обучения с подкреплением. 

# SNN
Обучена спайковая нейронная сеть с дофаминовым подкреплением для действий в этой же среде. 
<img width="795" alt="image" src="https://user-images.githubusercontent.com/42536677/183866393-fde31d07-095a-4d7e-97cf-fa7eeb210831.png">

Ноутбук расположен по ссылке:
https://colab.research.google.com/drive/1JL2tbDeryPjKrQ3pc52E4olLCdqhPONk?usp=sharing
